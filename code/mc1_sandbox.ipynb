{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis Sandbox\n",
    "\n",
    "This notebook is for playing with the Metropolis algorithm in the context of fitting a linear model. Tinker with how it works and see if you can make it fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a pre-prepared data file (or go off and make your own). See what the data look like.\n",
    "\n",
    "The pre-prepared data were produced from the model\n",
    "* $x \\sim \\mathrm{Normal}(e, 1)$\n",
    "* $y \\sim \\mathrm{Normal}(\\pi+\\phi\\,x, 1)$, where $\\phi$ is the Golden Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/mc1_sandbox.dat')\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(x, y, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be convenient to compare what we get from MCMC with the exact solution. Here is a class that packages that up (assuming uniform priors). Note that `a` and `b` need to be changed if you made up data according to a different model (as well as some of the `arange`s below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = np.pi, 1.6818\n",
    "class ExactPosterior:\n",
    "    def __init__(self, x, y, a0, b0):\n",
    "        X = np.matrix(np.vstack([np.ones(len(x)), x]).T)\n",
    "        Y = np.matrix(y).T\n",
    "        self.invcov = X.T * X\n",
    "        self.covariance = np.linalg.inv(self.invcov)\n",
    "        self.mean = self.covariance * X.T * Y\n",
    "        self.a_array = np.arange(0.0, 6.0, 0.02)\n",
    "        self.b_array = np.arange(0.0, 3.25, 0.02)\n",
    "        self.P_of_a = np.array([self.marg_a(a) for a in self.a_array])\n",
    "        self.P_of_b = np.array([self.marg_b(b) for b in self.b_array])\n",
    "        self.P_of_ab = np.array([[self.lnpost(a,b) for a in self.a_array] for b in self.b_array])\n",
    "        self.P_of_ab = np.exp(self.P_of_ab)\n",
    "        self.renorm = 1.0/np.sum(self.P_of_ab)\n",
    "        self.P_of_ab = self.P_of_ab * self.renorm\n",
    "        self.levels = scipy.stats.chi2.cdf(np.arange(4,1,-1)**2, 1) # confidence levels corresponding to contours below\n",
    "        self.contourLevels = self.renorm*np.exp(self.lnpost(a0,b0)-0.5*scipy.stats.chi2.ppf(self.levels, 2))\n",
    "    def lnpost(self, a, b): # the 2D posterior\n",
    "        z = self.mean - np.matrix([[a],[b]])\n",
    "        return -0.5 * (z.T * self.invcov * z)[0,0]\n",
    "    def marg_a(self, a): # marginal posterior of a\n",
    "        return scipy.stats.norm.pdf(a, self.mean[0,0], np.sqrt(self.covariance[0,0]))\n",
    "    def marg_b(self, b): # marginal posterior of b\n",
    "        return scipy.stats.norm.pdf(b, self.mean[1,0], np.sqrt(self.covariance[1,1]))\n",
    "exact = ExactPosterior(x, y, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo some plots of the exact posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(exact.a_array, exact.P_of_a); plt.xlabel('a');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.plot(exact.b_array, exact.P_of_b); plt.xlabel('b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 5.0)\n",
    "plt.contour(exact.a_array, exact.b_array, exact.P_of_ab, colors='blue', levels=exact.contourLevels);\n",
    "plt.plot(a, b, 'o', color='red'); plt.xlabel('a'); plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use uniform priors, for ease of comparison to the exact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnPrior(params):\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnLike(params, x, y):\n",
    "    a = params[0]\n",
    "    b = params[1]\n",
    "    return -0.5*np.sum((a+b*x - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package up a log-posterior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnPost(params, x, y):\n",
    "    return lnPrior(params) + lnLike(params, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sampler\n",
    "Improve as you see fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple Gaussian proposal distribution, for lack of any better ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propose(params, width):\n",
    "    return params + width * np.random.randn(params.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function for taking a step (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(current_params, current_lnP, width=1.0):\n",
    "    trial_params = propose(current_params, width=width)\n",
    "    trial_lnP = lnPost(trial_params, x, y)\n",
    "    if np.log(np.random.rand(1)) <= trial_lnP-current_lnP:\n",
    "        return (trial_params, trial_lnP)\n",
    "    else:\n",
    "        return (current_params, current_lnP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And away we go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set a random initial value on [-5,5] for each parameter, then run the sampler.\n",
    "\n",
    "`Nsamples` is set to 100 below, but you'll want to increase it after seeing that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = -5.0 + np.random.rand(2) * 10.0\n",
    "lnP = lnPost(params, x, y)\n",
    "\n",
    "Nsamples = 100\n",
    "samples = np.zeros((Nsamples, 2))\n",
    "\n",
    "for i in range(Nsamples):\n",
    "    params, lnP = step(params, lnP)\n",
    "    samples[i,:] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the chain in two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 7.0)\n",
    "plt.plot(samples[:,0], samples[:,1]);\n",
    "plt.plot(samples[0,0], samples[0,1], 'ro');\n",
    "plt.xlabel('a'); plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the *traces* of each parameter (vs time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 3.0)\n",
    "plt.plot(samples[:,0], 'o', ms=1.0); plt.ylabel('a');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 3.0)\n",
    "plt.plot(samples[:,1], 'o', ms=1.0); plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the basis of these diagnostics, we should identify a burn-in period to throw away hereafter. We could also *thin* the remaining chain to reduce the number of highly correlated and therefore redundant points.\n",
    "\n",
    "Here I've removed the first 500 points, but you should change this to whatever makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = samples[np.arange(501,Nsamples),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat earlier plots, \"zoomed in\" on the remaining samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7.0, 7.0)\n",
    "plt.plot(samples[:,0], samples[:,1]);\n",
    "plt.xlabel('a'); plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 3.0)\n",
    "plt.plot(samples[:,0], 'o', ms=1.0); plt.ylabel('a');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 3.0)\n",
    "plt.plot(samples[:,1], 'o', ms=1.0); plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the marginal and joint posterior distributions to the exact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.hist(samples[:,0], 20, normed=True, color='cyan');\n",
    "plt.plot(exact.a_array, exact.P_of_a, color='red');\n",
    "plt.xlabel('a');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.hist(samples[:,1], 20, normed=True, color='cyan');\n",
    "plt.plot(exact.b_array, exact.P_of_b, color='red');\n",
    "plt.xlabel('b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.plot(samples[:,0], samples[:,1], 'o', ms=1.0);\n",
    "plt.contour(exact.a_array, exact.b_array, exact.P_of_ab, colors='red', levels=exact.contourLevels);\n",
    "plt.xlabel('a'); plt.ylabel('b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play\n",
    "\n",
    "Mess around with the specifics of our Metropolis implementation above, in particular\n",
    "1. the initial state\n",
    "2. the proposal distribution width\n",
    "3. the chain length\n",
    "\n",
    "You could also insert code to keep track of the acceptance fraction (or compute it afterwards).\n",
    "\n",
    "In practice, we don't usually know where the best fit will be, and can't wait around while the posterior is evaluated an arbitrarily large number of times. With this in mind, brainstorm strategies for accelerating convergence. We'll come back to some concrete methods later in the course."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
