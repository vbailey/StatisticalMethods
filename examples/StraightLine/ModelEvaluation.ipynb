{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Straight Line Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "plt.rcParams['savefig.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from straightline_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "* Let's generate a simple Cepheids-like dataset: observations of $y$ with reported uncertainties $\\sigma_y$, at given $x$ values.\n",
    "\n",
    "\n",
    "> You can make a different dataset by choosing a different random, to see how the conclusions change from dataset to datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x,y,sigmay) = generate_data(seed=13)\n",
    "\n",
    "plot_yerr(x, y, sigmay)\n",
    "# plt.savefig(\"modelcheck-data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing the posterior PDF\n",
    "\n",
    "* Like we did in Session 2, we can evaluate the posterior PDF for the straight line model slope $m$ and intercept $b$ on a grid.\n",
    "\n",
    "\n",
    "* Let's also take some samples, with a simple Metropolis routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def straight_line_log_likelihood(theta, x, y, sigmay):\n",
    "    '''\n",
    "    Returns the log-likelihood of drawing data values *y* at\n",
    "    known values *x* given Gaussian measurement noise with standard\n",
    "    deviation with known *sigmay*, where the \"true\" y values are\n",
    "    *y_t = m * x + b*\n",
    "\n",
    "    x: list of x coordinates\n",
    "    y: list of y coordinates\n",
    "    sigmay: list of y uncertainties\n",
    "    m: scalar slope\n",
    "    b: scalar line intercept\n",
    "\n",
    "    Returns: scalar log likelihood\n",
    "    '''\n",
    "    m,b = theta\n",
    "    return (np.sum(np.log(1./(np.sqrt(2.*np.pi) * sigmay))) +\n",
    "            np.sum(-0.5 * (y - (m*x + b))**2 / sigmay**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def straight_line_log_prior(theta, theta_limits):\n",
    "    m, b = theta\n",
    "    mlimits, blimits = theta_limits\n",
    "    \n",
    "    # Uniform in m:\n",
    "    if (m < mlimits[0]) | (m > mlimits[1]):\n",
    "        log_m_prior = -np.inf\n",
    "    else:\n",
    "        log_m_prior = np.log(1.0/(mlimits[1] - mlimits[0]))\n",
    "    # Uniform in b:\n",
    "    if (b < blimits[0]) | (b > blimits[1]):\n",
    "        log_b_prior = -np.inf\n",
    "    else:\n",
    "        log_b_prior = np.log(1.0/(blimits[1] - blimits[0]))\n",
    "        \n",
    "    return log_m_prior + log_b_prior\n",
    "\n",
    "\n",
    "def straight_line_log_posterior(theta, x, y, sigmay, theta_limits):\n",
    "    return (straight_line_log_likelihood(theta, x, y, sigmay) +\n",
    "            straight_line_log_prior(theta, theta_limits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate log P(m,b | x,y,sigmay) on a grid.\n",
    "\n",
    "# Define uniform prior limits, enforcing positivity in both parameters:\n",
    "mlimits = [0.0, 2.0]\n",
    "blimits = [0.0, 200.0]\n",
    "theta_limits = (mlimits, blimits)\n",
    "\n",
    "# Set up grid:\n",
    "mgrid = np.linspace(mlimits[0], mlimits[1], 101)\n",
    "bgrid = np.linspace(blimits[0], blimits[1], 101)\n",
    "log_posterior = np.zeros((len(mgrid),len(bgrid)))\n",
    "\n",
    "# Evaluate log posterior PDF:\n",
    "for im,m in enumerate(mgrid):\n",
    "    for ib,b in enumerate(bgrid):\n",
    "        theta = (m,b)\n",
    "        log_posterior[im,ib] = straight_line_log_posterior(theta, x, y, sigmay, theta_limits)\n",
    "\n",
    "        \n",
    "# Convert to probability density and plot, taking care with very small values:\n",
    "\n",
    "posterior = np.exp(log_posterior - log_posterior.max())\n",
    "\n",
    "plt.imshow(posterior, extent=[blimits[0],blimits[1],mlimits[0],mlimits[1]],cmap='Blues',\n",
    "           interpolation='none', origin='lower', aspect=(blimits[1]-blimits[0])/(mlimits[1]-mlimits[0]),\n",
    "           vmin=0, vmax=1)\n",
    "plt.contour(bgrid, mgrid, posterior, pdf_contour_levels(posterior), colors='k')\n",
    "\n",
    "i = np.argmax(posterior)\n",
    "i,j = np.unravel_index(i, posterior.shape)\n",
    "print('Grid maximum posterior values (m,b) =', mgrid[i], bgrid[j])\n",
    "\n",
    "plt.title('Straight line: posterior PDF for parameters');\n",
    "plot_mb_setup(*theta_limits);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And now to draw some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metropolis(log_posterior, theta, theta_limits, stepsize, nsteps=10000):\n",
    "    '''\n",
    "    log_posterior: function of theta \n",
    "    theta_limits:  uniform prior ranges\n",
    "    stepsize:      scalar or vector proposal distribution width\n",
    "    nsteps:        desired number of samples\n",
    "    '''\n",
    "    \n",
    "    log_prob = log_posterior(theta, x, y, sigmay, theta_limits)\n",
    "    \n",
    "    # Store Markov chain as an array of samples:\n",
    "    chain = np.empty((nsteps, len(theta)))\n",
    "    log_probs = np.empty(nsteps)\n",
    "    \n",
    "    # Count our accepted proposals:\n",
    "    naccept = 0\n",
    "    \n",
    "    for i in range(nsteps):\n",
    "        \n",
    "        theta_new = theta + stepsize * np.random.randn(len(theta))\n",
    "        log_prob_new = log_posterior(theta_new, x, y, sigmay, theta_limits)\n",
    "\n",
    "        if np.log(np.random.rand()) < (log_prob_new - log_prob):\n",
    "            # accept, and move to the proposed position:\n",
    "            theta = theta_new\n",
    "            log_prob = log_prob_new\n",
    "            naccept += 1\n",
    "            \n",
    "        else:\n",
    "            # reject, and store the same sample as before:\n",
    "            pass\n",
    "        \n",
    "        chain[i] = theta\n",
    "        log_probs[i] = log_prob\n",
    "        \n",
    "    acceptance_rate = naccept/float(nsteps) \n",
    "    \n",
    "    return chain,log_probs,acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Round 1: Initialize m, b at the center of prior:\n",
    "# m = 0.5*(mlimits[0]+mlimits[1])\n",
    "# b = 0.5*(blimits[0]+blimits[1])\n",
    "# Round 2: Initialize m,b close to peak of posterior:\n",
    "m = 0.25\n",
    "b = 90.0\n",
    "\n",
    "theta = np.array([m,b])\n",
    "\n",
    "\n",
    "# Step sizes, 2% or 5% of the prior\n",
    "mstep = 0.02*(mlimits[1]-mlimits[0])\n",
    "bstep = 0.05*(blimits[1]-blimits[0])\n",
    "stepsize = np.array([mstep,bstep])        \n",
    "    \n",
    "# How many steps?\n",
    "nsteps = 10000\n",
    "   \n",
    "print('Running Metropolis Sampler for', nsteps, 'steps...')\n",
    "\n",
    "chain, log_probs, acceptance_rate = metropolis(\n",
    "    straight_line_log_posterior, theta, theta_limits, stepsize, nsteps=nsteps\n",
    ")\n",
    "\n",
    "print('Acceptance fraction:', acceptance_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull m and b arrays out of the Markov chain and plot them:\n",
    "mm = [m for m,b in chain]\n",
    "bb = [b for m,b in chain]\n",
    "\n",
    "# Traces, for convergence inspection:\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(mm, 'k-')\n",
    "plt.ylim(mlimits)\n",
    "plt.ylabel('Slope m', fontsize=16)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(bb, 'k-')\n",
    "plt.ylabel('Intercept b', fontsize=16)\n",
    "plt.ylim(blimits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looks reasonable: a short burn-in period, followed by reasonably well-mixed samples.\n",
    "\n",
    "\n",
    "* Now let's look at the samples in parameter space, overlaid on our gridded posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --no-deps corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "corner.corner(chain[5000:], labels=['m','b'], range=[mlimits,blimits],quantiles=[0.16,0.5,0.84],\n",
    "                show_titles=True, title_args={\"fontsize\": 12},\n",
    "                plot_datapoints=True, fill_contours=True, levels=[0.68, 0.95], \n",
    "                color='b', bins=80, smooth=1.0);\n",
    "# plt.savefig(\"modelcheck-linear-posterior.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* It looks like we made a nice, precise measurement! \n",
    "\n",
    "\n",
    "* We made a lot of assumptions though - so we now need to test them. Our next step is to check the model for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Checking\n",
    "\n",
    "* How do we know if our model is any good? One property that \"good\" models have is *accuracy.*\n",
    "\n",
    "\n",
    "* Accurate models generate data that is *like* the observed data. What does this mean? First we have to define what similarity is, in this context. \n",
    "\n",
    "\n",
    "* *Visual impression* is one very important way, that's usually best done first.\n",
    "\n",
    "\n",
    "* *Test statistics* that capture relevant features of the data are another. It's good to explore the posterior predictive distribution for these statistics, as a way of propagating our uncertainty in the model parameters into our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Model Checking\n",
    "\n",
    "* Plot realizations of the model in data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a straight line model for each parameter combination, and plot:\n",
    "X = np.linspace(xlimits[0],xlimits[1],50)\n",
    "for i in (np.random.rand(100)*len(chain)).astype(int):\n",
    "    m,b = chain[i]\n",
    "    plt.plot(X, b+X*m, 'b-', alpha=0.1)\n",
    "\n",
    "# Overlay the data, for comparison:\n",
    "plot_yerr(x, y, sigmay);\n",
    "\n",
    "# plt.savefig(\"modelcheck-linear.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Chi-squared Hypothesis Testing\n",
    "\n",
    "Let's look at a frequentist assessment of the goodness of this fit.\n",
    "\n",
    "The Maximum Likelihood Estimators (MLEs) for $m$ and $b$ can be obtained using `scipy.optimize.minimize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chisq(theta, x, y, sigmay):\n",
    "    m,b = theta\n",
    "    return np.sum((y - (m*x + b))**2 / sigmay**2)\n",
    "\n",
    "theta = np.array([m,b])\n",
    "\n",
    "import scipy.optimize\n",
    "result = scipy.optimize.minimize(chisq, theta, args=(x, y, sigmay))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ndof = len(y) - 2\n",
    "chisq_min = result.fun\n",
    "print(\"Minimum chisq = \",chisq_min,\", Ndof = \",Ndof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, where does this value of chisq fall in the $\\chi^2$ distribution with 28 degrees of freedom?\n",
    "\n",
    "The \"p-value\" gives the probability of getting this value, or higher, by chance, given that the model is true. If the p-value is less than some significance level (e.g. 0.05), then the null hypothesis (our straight line model) would be rejected.\n",
    "\n",
    "We can compute the p-value assuming a chi-squared distribution using `scipy.stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "x2 = scipy.stats.chi2(Ndof)\n",
    "pvalue = x2.sf(chisq_min)\n",
    "print(\"chisq_min = \",chisq_min,\"p-value = \",pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says that the null hypothesis can be rejected at the $10^{-10}$ significance level.\n",
    "\n",
    "The Fisher approximation to the $\\chi^2$ distribution provides a quick, useful and intuitive alternative to computing integrals over it directly. The approximation is that\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\sqrt{2\\chi^2_{\\rm min}} \\sim \\mathcal{N}\\left( \\sqrt{2 N_{\\rm dof}-1}, 1 \\right)$\n",
    "\n",
    "The difference between $\\sqrt{2\\chi^2_{\\rm min}}$ and $\\sqrt{2 N_{\\rm dof}-1}$ is the \"number of sigma\" we are away from a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(2*chisq_min) - np.sqrt(2*Ndof-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tests rely on the applicability of the standard $\\chi^2$ distribution to the problem. Let's now look at how this kind of hypothesis test extends to Bayesian model checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Model Checking\n",
    "\n",
    "* If our model is the true one, then *replica* data generated by it should look like the one dataset we have. This means that *summaries* of both the real dataset, $T(d)$, and the replica datasets, $T(d^{\\rm rep})$, should be drawn from the same distribution. \n",
    "\n",
    "\n",
    "* If the real dataset was not generated with our model, then its summary may be an outlier from the distribution of summaries of replica datasets.\n",
    "\n",
    "\n",
    "* We can account for our uncertainty in the parameters $\\theta$ by marginalizing them out, which can be easily done by just making the histogram of $T(d^{\\rm rep}(\\theta))$ from our posterior samples, after drawing one replica dataset $d^{\\rm rep}$ from the model sampling distribution ${\\rm Pr}(d^{\\rm rep}\\,|\\,\\theta)$ for each one.\n",
    "\n",
    "\n",
    "* Then, we can ask: what is the posterior probability for the summary $T$ to be greater than the observed summary $T(d)$? If this is very small or very large, we should be suspicious of our model - because it is not predicting the data very accurately.\n",
    "\n",
    "${\\rm Pr}(T(d^{\\rm rep})>T(d)\\,|\\,d) = \\int I(T(d^{\\rm rep})>T(d))\\,{\\rm Pr}(d^{\\rm rep}\\,|\\,\\theta)\\,{\\rm Pr}(\\theta\\,|\\,d)\\;d\\theta\\,dd^{\\rm rep}$\n",
    "\n",
    "* Here $I$ is the \"indicator function\" - 1 or 0 according to the condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test statistics: functions of the data, not the parameters.\n",
    "\n",
    "# 1) Weighted mean y:\n",
    "# def test_statistic(x,y,sigmay):\n",
    "#    return np.sum(y/sigmay**2.0)/np.sum(1.0/sigmay**2.0)\n",
    "\n",
    "# 2) Variance of y:\n",
    "# def test_statistic(x,y,sigmay):\n",
    "#    return np.var(y)\n",
    "\n",
    "# 3) Pearson r correlation coefficient:\n",
    "import scipy.stats\n",
    "def test_statistic(x,y,dummy):\n",
    "    '''\n",
    "    Pearson r correlation coefficient:\n",
    "    r12 = \\sum [(xi - xbar)*(yi - ybar)] / [\\sum (xi - xbar)^2 * \\sum (yi - ybar)^2]^1/2\n",
    "    '''\n",
    "    r12 = np.sum((x - np.mean(x))*(y - np.mean(y))) / \\\n",
    "          np.sqrt(np.sum((x - np.mean(x))**2) * np.sum((y - np.mean(y))**2))\n",
    "    return r12\n",
    "\n",
    "\n",
    "# Approximate the posterior predictive distribution for T, \n",
    "# by drawing a replica dataset for each sample (m,b) and computing its T:\n",
    "T = np.zeros(len(chain))\n",
    "for k,(m,b) in enumerate(chain):\n",
    "    yrep = b + m*x + np.random.randn(len(x)) * sigmay\n",
    "    T[k] = test_statistic(x,yrep,sigmay)\n",
    "    \n",
    "# Compare with the test statistic of the data, on a plot:   \n",
    "Td = test_statistic(x, y, sigmay)\n",
    "\n",
    "plt.hist(T, 100, histtype='step', color='blue', lw=2, range=(0.0,np.percentile(T,99.0)))\n",
    "plt.axvline(Td, color='black', linestyle='--', lw=2)\n",
    "plt.xlabel('Test statistic', fontsize=16)\n",
    "plt.ylabel('Posterior predictive distribution', fontsize=16)\n",
    "# plt.savefig(\"modelcheck-linear-TS.png\")\n",
    "\n",
    "# What is Pr(T>T(d)|d)?\n",
    "\n",
    "greater = (T > Td)\n",
    "P = 100*len(T[greater])/(1.0*len(T))\n",
    "print(\"Pr(T>T(d)|d) = \",P,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If our model is true (and we're just uncertain about its parameters, given the data), we can compute the probability of getting a $T$ less than that observed.\n",
    "\n",
    "\n",
    "* Note that we did not have to look up any particular standard distribution - we can simply compute the posterior predictive distribution given our generative model.\n",
    "\n",
    "\n",
    "* This test statistic lacks power: better choices might put more acute stress on the model to perform, by focusing on the places where the model predictions are suspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test statistics $T(d,\\theta)$ that are functions of both the data and the parameters are known as *discrepancy measures.* \n",
    "\n",
    "\n",
    "* Similar in spirit to the above, we can compute the posterior probability of getting $T(d^{\\rm rep},\\theta) > T(d,\\theta)$:\n",
    "\n",
    "${\\rm Pr}(T(d^{\\rm rep},\\theta)>T(d,\\theta)\\,|\\,d) = \\int I(T(d^{\\rm rep},\\theta)>T(d,\\theta))\\,{\\rm Pr}(d^{\\rm rep}\\,|\\,\\theta)\\,{\\rm Pr}(\\theta\\,|\\,d)\\;d\\theta\\,dd^{\\rm rep}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discrepancy: functions of the data AND parameters.\n",
    "\n",
    "# 1) Reduced chisq for the model:\n",
    "def discrepancy(x,y,sigmay,b,m):\n",
    "   return np.sum((y - m*x - b)**2.0/sigmay**2.0)/(len(y)-2)\n",
    "\n",
    "# Approximate the posterior predictive distribution for T, \n",
    "# by drawing a replica dataset for each sample (m,b) and computing its T, \n",
    "# AND ALSO its Td (which now depends on the parameters, too):\n",
    "T = np.zeros(len(chain))\n",
    "Td = np.zeros(len(chain))\n",
    "for k,(m,b) in enumerate(chain):\n",
    "    yrep = b + m*x + np.random.randn(len(x)) * sigmay\n",
    "    T[k] = discrepancy(x,yrep,sigmay,b,m)\n",
    "    Td[k] = discrepancy(x,y,sigmay,b,m)\n",
    "    \n",
    "# Compare T with Td, on a scatter plot - how often is T>Td?   \n",
    "\n",
    "plt.scatter(Td, T, color='blue',alpha=0.1)\n",
    "plt.plot([0.0, 100.0], [0.0, 100.], color='k', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Observed discrepancy $T(d,\\\\theta)$')\n",
    "plt.ylabel('Replicated discrepancy $T(d^{\\\\rm rep},\\\\theta)$')\n",
    "plt.ylim([0.0,np.percentile(Td,99.0)])\n",
    "plt.xlim([0.0,np.percentile(Td,99.0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of differences:\n",
    "\n",
    "diff = T - Td\n",
    "plt.hist(diff, 100, histtype='step', color='blue', lw=2, range=(np.percentile(diff,1.0),np.percentile(diff,99.0)))\n",
    "plt.axvline(0.0, color='black', linestyle='--', lw=2)\n",
    "plt.xlim(-5,1)\n",
    "plt.xlabel('Difference $T(d^{\\\\rm rep},\\\\theta) - T(d,\\\\theta)$', fontsize=16)\n",
    "plt.ylabel('Posterior predictive distribution', fontsize=16)\n",
    "# plt.savefig(\"modelcheck-linear-discrepancy.png\")\n",
    "\n",
    "# What is Pr(T>T(d)|d)?\n",
    "\n",
    "greater = (T > Td)\n",
    "Pline = 100*len(T[greater])/(1.0*len(T))\n",
    "print(\"Pr(T>T(d)|d) = \",Pline,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The conclusion drawn from the discrepancy is more interesting, in this case. All our $\\theta = (m,b)$ samples are plausible, so replica datasets generated by them should also be plausible. The straight line defined by each $(m,b)$ should go through the real data points as readily (*on average*) as it does its replica dataset.\n",
    "\n",
    "\n",
    "* Do our posterior predictive $p-$values suggest we need to improve our model? What about the visual check?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "* In some sense, the reduced chi-squared is actually not such an interesting test statistic, because it's very similar to the log likelihood! \n",
    "\n",
    "* Still more powerful discrepancy measures might stress-test *different* aspects of the model. Talk to your neighbor about where and how this model might be failing, and see if you can design a better discrepancy measure than reduced chi-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Expansion\n",
    "\n",
    "\n",
    "* Maybe I see some curvature in the data - or maybe I have a new astrophysical idea for how this data was generated. Let's try adding an extra parameter to the model, to make a quadratic function: \n",
    "\n",
    "$y = m x + b + q x^2$\n",
    "\n",
    "\n",
    "* The coefficient $q$ is probably pretty small (we were originally expecting to only have to use a straight line model for these data!), so I guess we can assign a fairly narrow prior, centered on zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quadratic_log_likelihood(theta, x, y, sigmay):\n",
    "    '''\n",
    "    Returns the log-likelihood of drawing data values y at\n",
    "    known values x given Gaussian measurement noise with standard\n",
    "    deviation with known sigmay, where the \"true\" y values are\n",
    "    y_t = m*x + b + q**2\n",
    "\n",
    "    x: list of x coordinates\n",
    "    y: list of y coordinates\n",
    "    sigmay: list of y uncertainties\n",
    "    m: scalar slope\n",
    "    b: scalar line intercept\n",
    "    q: quadratic term coefficient\n",
    "\n",
    "    where theta = (m, b, q)\n",
    "\n",
    "    Returns: scalar log likelihood\n",
    "    '''\n",
    "    m, b, q = theta\n",
    "    return (np.sum(np.log(1./(np.sqrt(2.*np.pi) * sigmay))) +\n",
    "            np.sum(-0.5 * (y - (m*x + b + q*x**2))**2 / sigmay**2))\n",
    "    \n",
    "    \n",
    "def quadratic_log_prior(theta, theta_limits):\n",
    "    m, b, q = theta\n",
    "    mlimits, blimits, qpars = theta_limits\n",
    "    \n",
    "    # m and b:\n",
    "    log_mb_prior = straight_line_log_prior(np.array([m,b]), np.array([mlimits, blimits]))\n",
    "    # q:\n",
    "    log_q_prior = np.log(1./(np.sqrt(2.*np.pi) * qpars[1])) - \\\n",
    "                  0.5 * (q - qpars[0])**2 / qpars[1]**2\n",
    "    return log_mb_prior + log_q_prior\n",
    "    \n",
    "    \n",
    "def quadratic_log_posterior(theta, x, y, sigmay, theta_limits):\n",
    "    return (quadratic_log_likelihood(theta, x, y, sigmay) +\n",
    "            quadratic_log_prior(theta, theta_limits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define uniform prior limits, enforcing positivity in m and b:\n",
    "mlimits = [0.0, 2.0]\n",
    "blimits = [0.0, 200.0]\n",
    "# Define Gaussian prior centered on zero for q:\n",
    "qpars = [0.0,0.003]\n",
    "\n",
    "# Round 1: Initialize m, b, q at the center of prior:\n",
    "# m = 0.5*(mlimits[0]+mlimits[1])\n",
    "# b = 0.5*(blimits[0]+blimits[1])\n",
    "# q = qpars[0]\n",
    "\n",
    "# Round 2: Initialize m, b, q close to peak of posterior:\n",
    "m = 0.9\n",
    "b = 20.0\n",
    "q = -0.002\n",
    "\n",
    "# Arrays to pass to the sampler:\n",
    "qtheta = np.array([m,b,q])\n",
    "qtheta_limits = (mlimits, blimits, qpars)\n",
    "\n",
    "# Step sizes, small fractions of the prior width:\n",
    "mstep = 0.02*(mlimits[1]-mlimits[0])\n",
    "bstep = 0.04*(blimits[1]-blimits[0])\n",
    "qstep = 0.04*qpars[1]\n",
    "stepsize = np.array([mstep,bstep,qstep])        \n",
    "    \n",
    "# How many steps?\n",
    "nsteps = 10000\n",
    "   \n",
    "print('Running Metropolis Sampler for', nsteps, 'steps...')\n",
    "\n",
    "qchain, log_probs, acceptance_rate = metropolis(\n",
    "    quadratic_log_posterior, qtheta, qtheta_limits, stepsize, nsteps=nsteps\n",
    ")\n",
    "\n",
    "print('Acceptance fraction:', acceptance_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull m, b and q arrays out of the Markov chain and plot them:\n",
    "mm = [m for m,b,q in qchain]\n",
    "bb = [b for m,b,q in qchain]\n",
    "qq = [q for m,b,q in qchain]\n",
    "\n",
    "# Traces, for convergence inspection:\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(mm, 'k-')\n",
    "plt.ylim(mlimits)\n",
    "plt.ylabel('Slope m')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(bb, 'k-')\n",
    "plt.ylim(blimits)\n",
    "plt.ylabel('Intercept b')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(qq, 'k-')\n",
    "plt.ylim([qpars[0]-3*qpars[1],qpars[0]+3*qpars[1]])\n",
    "plt.ylabel('Quadratic coefficient q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corner.corner(qchain, labels=['m','b','q'], range=[mlimits,blimits,(qpars[0]-3*qpars[1],qpars[0]+3*qpars[1])],quantiles=[0.16,0.5,0.84],\n",
    "                show_titles=True, title_args={\"fontsize\": 12},\n",
    "                plot_datapoints=True, fill_contours=True, levels=[0.68, 0.95], \n",
    "                color='green', bins=80, smooth=1.0);\n",
    "# plt.savefig(\"modelcheck-quadratic-posterior.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All parameters are again precisely constrained.\n",
    "\n",
    "\n",
    "* The gradient and intercept $m$ and $b$ are significantly different from before, though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Quadratic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Posterior visual check, in data space:\n",
    "X = np.linspace(xlimits[0],xlimits[1],100)\n",
    "for i in (np.random.rand(100)*len(chain)).astype(int):\n",
    "    m,b,q = qchain[i]\n",
    "    plt.plot(X, b + X*m + q*X**2, 'g-', alpha=0.1)\n",
    "plot_yerr(x, y, sigmay)\n",
    "# plt.savefig(\"modelcheck-quadratic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discrepancy: functions of the data AND parameters.\n",
    "\n",
    "# 1) Reduced chisq for the model:\n",
    "def discrepancy(x,y,sigmay,m,b,q):\n",
    "   return np.sum((y - m*x - b - q*x**2)**2.0/sigmay**2.0)/(len(y)-3)\n",
    "\n",
    "\n",
    "# Approximate the posterior predictive distribution for T, \n",
    "# by drawing a replica dataset for each sample (m,b) and computing its T, \n",
    "# AND ALSO its Td:\n",
    "T = np.zeros(len(qchain))\n",
    "Td = np.zeros(len(qchain))\n",
    "for k,(m,b,q) in enumerate(qchain):\n",
    "    yp = b + m*x + q*x**2 + sigmay*np.random.randn(len(x))\n",
    "    T[k] = discrepancy(x,yp,sigmay,m,b,q)\n",
    "    Td[k] = discrepancy(x,y,sigmay,m,b,q)\n",
    "\n",
    "# Histogram of differences:\n",
    "diff = T - Td\n",
    "plt.hist(diff, 100, histtype='step', color='green', lw=2, range=(np.percentile(diff,1.0),np.percentile(diff,99.0)))\n",
    "plt.axvline(0.0, color='black', linestyle='--', lw=2)\n",
    "plt.xlabel('Difference $T(d^{\\\\rm rep},\\\\theta) - T(d,\\\\theta)$', fontsize=16)\n",
    "plt.ylabel('Posterior predictive distribution', fontsize=16)\n",
    "# plt.savefig(\"modelcheck-quadratic-discrepancy.png\")\n",
    "\n",
    "# What is Pr(T>T(d)|d)?\n",
    "greater = (T > Td)\n",
    "Pquad = 100*len(T[greater])/(1.0*len(T))\n",
    "print(\"Pr(T>T(d)|d,quadratic) = \",Pquad,\"%, cf. Pr(T>T(d)|d,straightline) = \",Pline,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do the two models compare? Which one matches the data better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison with the Evidence\n",
    "\n",
    "* In the posterior predictive check above we found it useful to consider the probability distribution of hypothetical replica datasets. \n",
    "\n",
    "\n",
    "* As well as this, we also have access to the quantity ${\\rm Pr}(d\\,|\\,H)$, the evidence for the model $H$. [Let's take a look at its properties](../../notes/Evidence.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Evidence\n",
    "\n",
    "* The FML is in general quite difficult to calculate, since it involves averaging the likelihood over the prior. MCMC gives us samples from the posterior - and these cannot, it turns out, be reprocessed so as to estimate the evidence stably.\n",
    "\n",
    "\n",
    "* A number of sampling algorithms have been developed that *do* calculate the evidence, during the process of sampling. These include:\n",
    "\n",
    "  * Nested Sampling (including MultiNest and DNest)\n",
    "  * Parallel Tempering, Thermodynamic Integration\n",
    "  * ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we draw samples from the *prior*, we can then estimate the evidence via the usual sum over samples,\n",
    "\n",
    "${\\rm Pr}(d\\,|\\,H) \\approx \\frac{1}{N_s} \\sum_k\\;{\\rm Pr}(d\\,|\\,\\theta_s,H)$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Simple Monte Carlo: \n",
    "> Consider the simplest possbile posterior inference, the mean of parameter $\\theta$:\n",
    ">  \n",
    "> $\\bar{\\theta} = \\int\\,\\theta\\,{\\rm Pr}(\\theta\\,|\\,d,H)\\,d\\theta$\n",
    "> \n",
    "> This is approximated by an average over posterior samples:\n",
    ">\n",
    "> $\\bar{\\theta} \\approx \\frac{1}{N_s}\\,\\sum_s\\,\\theta_s$\n",
    ">\n",
    "> Now replace the posterior PDF with Bayes' Theorem:\n",
    ">\n",
    "> $\\bar{\\theta} = \\frac{1}{Z} \\int\\,\\theta\\,{\\rm Pr}(d\\,|\\,\\theta,H)\\,{\\rm Pr}(\\theta\\,|\\,H)\\,d\\theta$\n",
    ">\n",
    "> and the posterior samples with prior samples:\n",
    "> \n",
    "> $\\bar{\\theta} \\approx \\frac{1}{(Z N_s)}\\,\\sum_s\\,\\theta_s\\,{\\rm Pr}(d\\,|\\,\\theta_s,H)$\n",
    ">\n",
    "> This is a *weighted mean* - if we don't want $Z$, we just define weights $w_s = {\\rm Pr}(d\\,|\\,\\theta_s,H)$ compute the approximate posterior mean as the likelihood-weighted prior mean:\n",
    "> \n",
    "> $\\bar{\\theta} \\approx \\frac{\\sum_s\\,w_s\\,\\theta_s}{\\sum_s\\,w_s}$\n",
    ">\n",
    "> To compute Z, we multiply both sides by $Z$ and replace $\\theta$ with 1:\n",
    ">\n",
    "> $Z \\approx \\frac{1}{N_s} \\sum_s\\,\\,w_s$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple Monte Carlo works well in certain low-dimensional situations, but in general it is very inefficient (at best). \n",
    "\n",
    "\n",
    "* Still, let's give it a try on our two models, and attempt to compute the evidence ratio \n",
    "  \n",
    "$R = \\frac{{\\rm Pr}(d\\,|\\,{\\rm quadratic})}{{\\rm Pr}(d\\,|\\,{\\rm straight line})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw a large number of prior samples and calculate the log likelihood for each one:\n",
    "N = 50000\n",
    "\n",
    "# Set the priors:\n",
    "mlimits = [0.0, 2.0]\n",
    "blimits = [0.0, 200.0]\n",
    "qpars = [0.0,0.003]\n",
    "\n",
    "# Sample from the prior:\n",
    "mm = np.random.uniform(mlimits[0],mlimits[1], size=N)\n",
    "bb = np.random.uniform(blimits[0],blimits[1], size=N)\n",
    "qq = qpars[0] + qpars[1]*np.random.randn(N)\n",
    "\n",
    "# We'll store the posterior samples as a \"chain\" again\n",
    "schain = []\n",
    "\n",
    "log_likelihood_straight_line = np.zeros(N)\n",
    "log_likelihood_quadratic = np.zeros(N)\n",
    "for i in range(N):\n",
    "    theta = np.array([mm[i], bb[i]])\n",
    "    log_likelihood_straight_line[i] = straight_line_log_likelihood(theta, x, y, sigmay)\n",
    "    qtheta = np.array([mm[i], bb[i], qq[i]])\n",
    "    log_likelihood_quadratic[i] = quadratic_log_likelihood(qtheta, x, y, sigmay)    \n",
    "\n",
    "    schain.append((mm[i],bb[i],qq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the log likelihood for each sample, let's check that we did actually sample the posterior well. Here are the corner plots (note that for plotting, the weights don't need to be correctly normalized - and also that we do *not* want to plot the points as well as the contours, since the points are prior samples not posterior ones!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unnormalized likelihoods for plotting:\n",
    "\n",
    "unnormalized_likelihood_straight_line = np.exp(log_likelihood_straight_line - log_likelihood_straight_line.max())\n",
    "unnormalized_likelihood_quadratic = np.exp(log_likelihood_quadratic - log_likelihood_quadratic.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corner.corner(schain, labels=['m','b','q'], range=[mlimits,blimits,(qpars[0]-3*qpars[1],qpars[0]+3*qpars[1])],quantiles=[0.16,0.5,0.84],\n",
    "                weights=unnormalized_likelihood_straight_line,\n",
    "                show_titles=True, title_args={\"fontsize\": 12},\n",
    "                plot_datapoints=False, fill_contours=True, levels=[0.68, 0.95], \n",
    "                color='blue', bins=80, smooth=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "* Why does $q$ appear in this plot? Does it affect the evidence calculation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corner.corner(schain, labels=['m','b','q'], range=[mlimits,blimits,(qpars[0]-3*qpars[1],qpars[0]+3*qpars[1])],quantiles=[0.16,0.5,0.84],\n",
    "                weights=unnormalized_likelihood_quadratic,\n",
    "                show_titles=True, title_args={\"fontsize\": 12},\n",
    "                plot_datapoints=False, fill_contours=True, levels=[0.68, 0.95], \n",
    "                color='green', bins=80, smooth=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the evidence - we'll need a special function that stably calculates the average $x$ given an array of $\\log x$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logaverage(x):\n",
    "    mx = x.max()\n",
    "    return np.log(np.sum(np.exp(x - mx))) + mx - np.log(len(x))\n",
    "\n",
    "log_evidence_straight_line = logaverage(log_likelihood_straight_line)\n",
    "log_evidence_quadratic = logaverage(log_likelihood_quadratic)\n",
    "\n",
    "print('log Evidence for Straight Line Model:', log_evidence_straight_line)\n",
    "print('log Evidence for Quadratic Model:', log_evidence_quadratic)\n",
    "\n",
    "print('Evidence ratio in favour of the Quadratic Model:', np.int(np.exp(log_evidence_quadratic - log_evidence_straight_line)),\"to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Play around with the Simple Monte Carlo evidence estimate, and see if you can break it. \n",
    "\n",
    "* Try running it a few times, with the same `N`: how precise are these evidence estimates from 50000 samples?\n",
    "\n",
    "* We said we believed *a priori* that $q$ should be small, and set the prior width accordingly. What would happen if we started to doubt ourselves, and re-assigned a broader prior? Try running with some larger values for the Gaussian prior width $q[1]$, and note what happens to the Bayes Factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* The Bayesian evidence is *qualitatively different* from other model assessments. While they focus primarily on *prediction accuracy,* the evidence is the way in which information from the prior PDF propagates through into our posterior beliefs about the model as a whole.\n",
    "\n",
    "\n",
    "* There are no mathematical limitations to its use[[*citation needed*]](), in contrast to various other hypothesis tests that are only valid under certain assumptions (such as the models being nested). Any two models can be compared and the odds ratio computed.\n",
    "\n",
    "\n",
    "* Jeffreys provided a scale to aid the interpretation of Bayes Factors:\n",
    "\n",
    "| **R**  | **Strength of evidence**|\n",
    "|--------|:------------------------|\n",
    "|  < 1   | Negative |\n",
    "|  1-3   | Barely worth mentioning |\n",
    "|  3-10  | Substantial |\n",
    "| 10-30  | Strong |\n",
    "| 30-100 | Very strong |\n",
    "|   >100 | Decisive\n",
    "\n",
    "Note that evidence ratios can be interpreted in very similar ways to *odds* at the Bookmakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Criteria\n",
    "\n",
    "A number of \"information criteria\" exist in the statistical literature, as *easier-to-calculate alternatives* to the Bayesian Evidence. They all have the form:\n",
    "\n",
    "$XIC(H) = -2\\log \\hat{L} + C$\n",
    "\n",
    "* Here, the $\\hat{L}$ term captures the goodness of fit of the model $H$, while the constant $C$ increases with model complexity. Models with *low* $XIC$ are preferred - for having high goodness of fit at low model complexity. Popular choices are the Bayesian Information Criterion $BIC$, where $C = K \\log N$, and the corrected Akaike Information Criterion $AICc$, where $C = 2K + 2K(K+1)/(N-K-1)$. Both take the maximum likelihood value as $\\hat{L}$.\n",
    "\n",
    "\n",
    "* Their derivation is different: $BIC$ is supposed to approximate the log evidence (in the limit of good data, linear model, and uniform priors), while differences in $AIC$ approximate the relative information loss when using the model to approximate the actual data generator. $AIC$ focuses more on model accuracy. \n",
    "\n",
    "\n",
    "* In general differences in $XIC$ are not easy to interpret, but tables similar to the Jeffreys scale exist. In general, testing against realistic simulated data on a case by case basis will probably provide the best guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Keep in Mind\n",
    "\n",
    "Here are some good things to keep in mind when using, or reading about, the evidence for a model: \n",
    "\n",
    "\n",
    "  * The evidence is only linearly sensitive to prior volume, but exponentially sensitive to goodness of fit. One way to interpret this that if you want a clearer sense of which model makes the data more probable, *get more data.*\n",
    "  \n",
    "  \n",
    "  * This piece of common sense is reflected by both the probability theory and the behaviour of scientists: most model comparison in practice is done *informally*, with approximate quantifications simply showing what signal to noise regime we are in. If we are in the low signal to noise regime, model comparison typically proceeds via discussion in the literature rather than mathematical test.\n",
    "  \n",
    "\n",
    "  * If you don't believe your priors, or indeed your models, then it may not be very meaningful to compare evidences across models: it can be a distraction from things you care about more - such as accuracy, or cost. \n",
    "  \n",
    "  \n",
    "  * An oft-used phrase in such discussions is \"garbage in, garbage out.\" You should pay attention to this: the evidence is the $K$-dimensional marginalization integral of the likelihood over the prior, while most of the numerical results you put in your abstracts are $K-1$-dimensional marginalization integrals. The degree to which you trust your evidence values should be commensurate with the degree to which you trust your other inferences. \n",
    "  \n",
    "  \n",
    "  * A reliably-computed evidence integral is a good indication of an accurately characterised posterior PDF. Indeed, some high performance sampling methods (such as nested sampling) were developed specifically *in order to compute the evidence accurately*.\n",
    "  \n",
    "  \n",
    "  * The evidence appears in *second-level inferences*, when parts of your model that you previously considered to be constant now need to be varied and inferred. The FML is only one parameter liberation away from becoming a likelihood.\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
