{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Period - Magnitude Relation in Cepheid Stars\n",
    "\n",
    "* Cepheids are stars whose brightness oscillates with a stable period that appears to be strongly correlated with their luminosity (or absolute magnitude).\n",
    "\n",
    "\n",
    "* A lot of _monitoring_ data - repeated imaging and subsequent \"photometry\" of the star - can provide a measurement of the absolute magnitude (if we know the distance to it's host galaxy) and the period of the oscillation.\n",
    "\n",
    "\n",
    "* Let's look at some Cepheid measurements reported by [Riess et al (2011)](Riess et al., 2011, ApJ, 730, 119).  Like the correlation function summaries, they are in the form of datapoints with error bars, where it is not clear how those error bars were derived (or what they mean).\n",
    "\n",
    "\n",
    "* Our goal is to infer the parameters of a simple relationship between Cepheid period and, in the first instance, apparent magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Look at Each Host Galaxy's Cepheids\n",
    "\n",
    "Let's read in all the data, and look at each galaxy's Cepheid measurements separately. Instead of using `pandas`, we'll write our own simple data structure, and give it a custom plotting method so we can compare the different host galaxies' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we need to know what's in the data file.\n",
    "\n",
    "!head -30 R11ceph.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Cepheids(object):\n",
    "    \n",
    "    def __init__(self,filename):\n",
    "        # Read in the data and store it in this master array:\n",
    "        self.data = np.loadtxt(filename)\n",
    "        self.hosts = self.data[:,1].astype('int').astype('str')\n",
    "        # We'll need the plotting setup to be the same each time we make a plot:\n",
    "        colornames = ['red','orange','yellow','green','cyan','blue','violet','magenta','gray']\n",
    "        self.colors = dict(zip(self.list_hosts(), colornames))\n",
    "        self.xlimits = np.array([0.3,2.3])\n",
    "        self.ylimits = np.array([30.0,17.0])\n",
    "        return\n",
    "    \n",
    "    def list_hosts(self):\n",
    "        # The list of (9) unique galaxy host names:\n",
    "        return np.unique(self.hosts)\n",
    "    \n",
    "    def select(self,ID):\n",
    "        # Pull out one galaxy's data from the master array:\n",
    "        index = (self.hosts == str(ID))\n",
    "        self.mobs = self.data[index,2]\n",
    "        self.merr = self.data[index,3]\n",
    "        self.logP = np.log10(self.data[index,4])\n",
    "        return\n",
    "    \n",
    "    def plot(self,X):\n",
    "        # Plot all the points in the dataset for host galaxy X.\n",
    "        ID = str(X)\n",
    "        self.select(ID)\n",
    "        plt.rc('xtick', labelsize=16) \n",
    "        plt.rc('ytick', labelsize=16)\n",
    "        plt.errorbar(self.logP, self.mobs, yerr=self.merr, fmt='.', ms=7, lw=1, color=self.colors[ID], label='NGC'+ID)\n",
    "        plt.xlabel('$\\\\log_{10} P / {\\\\rm days}$',fontsize=20)\n",
    "        plt.ylabel('${\\\\rm magnitude (AB)}$',fontsize=20)\n",
    "        plt.xlim(self.xlimits)\n",
    "        plt.ylim(self.ylimits)\n",
    "        plt.title('Cepheid Period-Luminosity (Riess et al 2011)',fontsize=20)\n",
    "        return\n",
    "\n",
    "    def overlay_straight_line_with(self,a=0.0,b=24.0):\n",
    "        # Overlay a straight line with gradient a and intercept b.\n",
    "        x = self.xlimits\n",
    "        y = a*x + b\n",
    "        plt.plot(x, y, 'k-', alpha=0.5, lw=2)\n",
    "        plt.xlim(self.xlimits)\n",
    "        plt.ylim(self.ylimits)\n",
    "        return\n",
    "    \n",
    "    def add_legend(self):\n",
    "        plt.legend(loc='upper left')\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = Cepheids('R11ceph.dat')\n",
    "print(data.colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we are all set up! Let's plot one of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(4258)\n",
    "\n",
    "# for ID in data.list_hosts():\n",
    "#     data.plot(ID)\n",
    "    \n",
    "data.overlay_straight_line_with(a=-2.0,b=24.0)\n",
    "\n",
    "data.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  Q: Is the Cepheid Period-Luminosity relation likely to be well-modeled by a power law ?\n",
    "\n",
    "Is it easy to find straight lines that \"fit\" all the data from each host? And do we get the same \"fit\" for each host?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the Period-Magnitude Relation\n",
    "\n",
    "* Let's try inferring the parameters $a$ and $b$ of the following linear relation:\n",
    "\n",
    "$m = a\\;\\log_{10} P + b$\n",
    "\n",
    "* We have data consisting of *observed magnitudes with quoted uncertainties*, of the form \n",
    "\n",
    "$m^{\\rm obs} = 24.51 \\pm 0.31$ at $\\log_{10} P = \\log_{10} (13.0/{\\rm days})$\n",
    "\n",
    "* Let's draw a PGM for this, imagining our way  through what we would do to generate a mock dataset like the one we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"../../graphics/pgms_cepheids.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What are reasonable assumptions about the sampling distribution for the $k^{\\rm th}$ datapoint, ${\\rm Pr}(m^{\\rm obs}_k|m_k,H)$?\n",
    "\n",
    "* We were given points ($m^{\\rm obs}_k$) with error bars ($\\sigma_k$), which suggests a *Gaussian* sampling distribution (as was suggested in Session 1):\n",
    "\n",
    "${\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H) = \\frac{1}{Z} \\exp{-\\frac{(m^{\\rm obs}_k - m_k)^2}{2\\sigma_k^2}}$\n",
    "\n",
    "\n",
    "* Then, we might suppose that the measurements of each Cepheid start are *independent* of each other, so that we can define *predicted and observed data vectors* $m$ and $m^{\\rm obs}$ (plus a corresponding observational uncertainty vector $\\sigma$) via:\n",
    "\n",
    "${\\rm Pr}(m^{\\rm obs}|m,\\sigma,H) = \\prod_k {\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the conditional PDF ${\\rm Pr}(m_k|a,b,\\log{P_k},H)$?\n",
    "\n",
    "Our relationship between the intrinsic magnitude and the log period is linear and deterministic, indicating the following *delta-function* PDF:\n",
    "\n",
    "${\\rm Pr}(m_k|a,b,\\log{P_k},H) = \\delta(m_k - a\\log{P_k} - b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the resulting joint likelihood, ${\\rm Pr}(m^{\\rm obs}|a,b,H)$?\n",
    "\n",
    "* The factorisation of the joint PDF for everything inside the plate that is illustrated by the PGM is:\n",
    "\n",
    "${\\rm Pr}(m^{\\rm obs}|m,\\sigma,H)\\;{\\rm Pr}(m|a,b,H) = \\prod_k {\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H)\\;\\delta(m_k - a\\log{P_k} - b)$\n",
    "\n",
    "* The intrinsic magnitudes of each Cepheid ($m$) are not interesting, and so we marginalize them out:\n",
    "\n",
    "${\\rm Pr}(m^{\\rm obs}|a,b,H) = \\int {\\rm Pr}(m^{\\rm obs}|m,\\sigma,H)\\;{\\rm Pr}(m|a,b,H)\\; dm$\n",
    "\n",
    "so that ${\\rm Pr}(m^{\\rm obs}|a,b,H) = \\prod_k {\\rm Pr}(m^{\\rm obs}_k|[a\\log{P_k} + b],\\sigma,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the log likelihood?\n",
    "\n",
    "$\\log {\\rm Pr}(m^{\\rm obs}|a,b,H) = \\sum_k \\log {\\rm Pr}(m^{\\rm obs}_k|[a\\log{P_k} + b],\\sigma,H)$\n",
    "\n",
    "which, substituting in our Gaussian form, gives us: \n",
    "\n",
    "$\\log {\\rm Pr}(m^{\\rm obs}|a,b,H) = {\\rm constant} - 0.5 \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$\n",
    "\n",
    "* This sum is often called $\\chi^2$ (\"chi-squared\"), and you may have seen it before. It's an effective \"misfit\" statistic, quantifying the difference between observed and predicted data - and under the assumptions outlined here, it's twice the log likelihood (up to a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What could be reasonable assumptions for the prior ${\\rm Pr}(a,b|H)$?\n",
    "\n",
    "For now, we can (continue to) assume a uniform distribution for each of $a$ and $b$ - in the homework, you can investigate some alternatives.\n",
    "\n",
    "${\\rm Pr}(a|H) = \\frac{1.0}{a_{\\rm max} - a_{\\rm min}}\\;\\;{\\rm for}\\;\\; a_{\\rm min} < a < a_{\\rm max}$\n",
    "\n",
    "\n",
    "${\\rm Pr}(b|H) = \\frac{1.0}{b_{\\rm max} - b_{\\rm min}}\\;\\;{\\rm for}\\;\\; b_{\\rm min} < b < b_{\\rm max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to code up functions for the log likelihood, log prior and log posterior, such that we can evaluate them on a 2D parameter grid. Let's fill them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood(logP,mobs,merr,a,b):\n",
    "    return -0.5*np.sum((mobs - a*logP -b)**2/(merr**2))\n",
    "\n",
    "def log_prior(a,b):\n",
    "    amin,amax = -10.0,10.0\n",
    "    bmin,bmax = 10.0,30.0\n",
    "    if (a > amin)*(a < amax)*(b > bmin)*(b < bmax):\n",
    "        logp = np.log(1.0/(amax-amin)) + np.log(1.0/(bmax-bmin))\n",
    "    else:\n",
    "        logp = -np.inf\n",
    "    return logp\n",
    "\n",
    "def log_posterior(logP,mobs,merr,a,b):\n",
    "    return log_likelihood(logP,mobs,merr,a,b) + log_prior(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up a suitable parameter grid and compute the posterior PDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select a Cepheid dataset:\n",
    "data.select(4258)\n",
    "\n",
    "# Set up parameter grids:\n",
    "npix = 100\n",
    "amin,amax = -4.0,-2.0\n",
    "bmin,bmax = 25.0,27.0\n",
    "agrid = np.linspace(amin,amax,npix)\n",
    "bgrid = np.linspace(bmin,bmax,npix)\n",
    "logprob = np.zeros([npix,npix])\n",
    "\n",
    "# Loop over parameters, computing unnormlized log posterior PDF:\n",
    "for i,a in enumerate(agrid):\n",
    "    for j,b in enumerate(bgrid):\n",
    "        logprob[j,i] = log_posterior(data.logP,data.mobs,data.merr,a,b)\n",
    "\n",
    "# Normalize and exponentiate to get posterior density:\n",
    "Z = np.max(logprob)\n",
    "prob = np.exp(logprob - Z)\n",
    "norm = np.sum(prob)\n",
    "prob /= norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot, with confidence contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted = np.sort(prob.flatten())\n",
    "C = sorted.cumsum()\n",
    "\n",
    "# Find the pixel values that lie at the levels that contain\n",
    "# 68% and 95% of the probability:\n",
    "lvl68 = np.min(sorted[C > (1.0 - 0.68)])\n",
    "lvl95 = np.min(sorted[C > (1.0 - 0.95)])\n",
    "\n",
    "plt.imshow(prob, origin='lower', cmap='Blues', interpolation='none', extent=[amin,amax,bmin,bmax])\n",
    "plt.contour(prob,[lvl68,lvl95],colors='black',extent=[amin,amax,bmin,bmax])\n",
    "plt.grid()\n",
    "plt.xlabel('slope a')\n",
    "plt.ylabel('intercept b / AB magnitudes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Are these inferred parameters sensible? \n",
    "\n",
    "\n",
    "* Let's read off a plausible (a,b) pair and overlay the model period-magnitude relation on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(4258)\n",
    "\n",
    "data.overlay_straight_line_with(a=-3.0,b=26.3)\n",
    "\n",
    "data.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this looks good! Later in the course we will do some more extensive *model checking*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing our Inferences\n",
    "\n",
    "Let's compute the 1D marginalized posterior PDFs for $a$ and for $b$, and report the median and \"68% credible interval\" (defined as the region of 1D parameter space enclosing 68% of the posterior probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_a_given_data = np.sum(prob,axis=0) # Approximate the integral as a sum\n",
    "prob_b_given_data = np.sum(prob,axis=1) # Approximate the integral as a sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(prob_a_given_data.shape, np.sum(prob_a_given_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 1D distributions:\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(15, 6)\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "left = ax[0].plot(agrid, prob_a_given_data)\n",
    "ax[0].set_title('${\\\\rm Pr}(a|d)$')\n",
    "ax[0].set_xlabel('slope $a$')\n",
    "ax[0].set_ylabel('Posterior probability density')\n",
    "\n",
    "right = ax[1].plot(bgrid, prob_b_given_data)\n",
    "ax[1].set_title('${\\\\rm Pr}(a|d)$')\n",
    "ax[0].set_xlabel('intercept $b$ / AB magnitudes')\n",
    "ax[1].set_ylabel('Posterior probability density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compress each PDF into a median and 68% credible interval, and report:\n",
    "\n",
    "def compress_1D_pdf(x,pr,ci=68,dp=1):\n",
    "    \n",
    "    # Interpret credible interval request:\n",
    "    low  = (1.0 - ci/100.0)/2.0    # 0.16 for ci=68\n",
    "    high = 1.0 - low               # 0.84 for ci=68\n",
    "\n",
    "    # Find cumulative distribution and compute percentiles:\n",
    "    cumulant = pr.cumsum()\n",
    "    pctlow = x[cumulant>low].min()\n",
    "    median = x[cumulant>0.50].min()\n",
    "    pcthigh = x[cumulant>high].min()\n",
    "    \n",
    "    # Convert to error bars, and format a string:\n",
    "    errplus = np.abs(pcthigh - median)\n",
    "    errminus = np.abs(median - pctlow)\n",
    "    \n",
    "    report = \"$ \"+str(round(median,dp))+\"^{+\"+str(round(errplus,dp))+\"}_{-\"+str(round(errminus,dp))+\"} $\"\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"a = \",compress_1D_pdf(agrid,prob_a_given_data,ci=68,dp=2))\n",
    "\n",
    "print(\"b = \",compress_1D_pdf(bgrid,prob_b_given_data,ci=68,dp=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* In this simple case, our report makes sense: the medians of both 1D marginalized PDFs lie within the region of high 2D posterior PDF. *This will not always be the case.*\n",
    "\n",
    "\n",
    "* The marginalized posterior for $x$ has a well-defined meaning, regardless of the higher dimensional structure of the joint posterior:  it is ${\\rm Pr}(x|d,H)$, the PDF for $x$ given the data and the model, and *accounting for the uncertainty in all other parameters*.\n",
    "\n",
    "\n",
    "* The high degree of symmetry in this problem is due to the posterior being a *bivariate Gaussian.* We could have derived the posterior PDF *analytically* - but in general this will not be possible. The homework invites you to explore various other analytic and numerical possibilities in this simple inference scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
