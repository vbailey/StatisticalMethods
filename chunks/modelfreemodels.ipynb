{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# \"Model-free\" Models\n",
    "\n",
    "Goals:\n",
    "* Introduce and use techniques that purport to be \"model independent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* Ivezic 6.3\n",
    "* Gelman ch. 18\n",
    "* Rasmussen & Williams [*Gaussian Processes for Machine Learning*](http://www.gaussianprocess.org/gpml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is \"model-free\"?\n",
    "\n",
    "Sometimes we simply don't have a good first-principles model for what's going on in our data, but we're also confident that making a simple assumption (e.g. Gaussian scatter) is dead wrong.\n",
    "\n",
    "Examples:\n",
    "* Photometric redshifts (catastrophic errors)\n",
    "* Photometric supernova detections (multiple populations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is \"model-free\"?\n",
    "\n",
    "In these situations, we're motivated to avoid strong modeling assumptions and instead be more empirical.\n",
    "\n",
    "Common adjectives:\n",
    "* non-parametric\n",
    "* model-independent\n",
    "* data-driven\n",
    "* empirical\n",
    "\n",
    "(Strictly speaking, these tend to correspond to models with very many parameters, but the terminology persists.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's here\n",
    "* Resampling methods\n",
    "* Mixture models\n",
    "* \"Non-parametric\" models and stochastic processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resampling methods: jackknife and bootstrap\n",
    "\n",
    "These methods try to compensate for \"small sample\" effects in estimation. The classical example is the sample average in the presence of a heavy-tailed scatter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jackknife procedure\n",
    "\n",
    "1. Remove 1 (or more) data points from the data set.\n",
    "2. Calculate the estimate of interest using the reduced data set.\n",
    "3. Repeat this for every possible reduced data set.\n",
    "\n",
    "The average (compared to the full-data-set calculation) and scatter of these estimates provides some idea of the small-sample bias.\n",
    "\n",
    "(Note that our CMB colleagues have invented an unrelated test that they like to call a jackknife. Don't get confused!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrap\n",
    "\n",
    "The bootstrap is a little more sophisticated. The idea is that we have data that sample a distribution, so they can be used as a direct (if crude) estimate of that distribution without further assumptions. A key requirement is that the measured data are a fair representation of what we might have gotten.\n",
    "\n",
    "1. Generate a new data set of the same size as the real data by sampling **with replacement** from the real data points.\n",
    "2. Calculate whatever statistic or estimate is of interest from the bootstrap data set.\n",
    "3. Do this many times, and interpret the resulting distribution as indicative of the true uncertainty in the measurement.\n",
    "\n",
    "Again, the classic example is estimating a sample mean or unweighted regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrap variants: parametric\n",
    "\n",
    "Instead of resampling data points, each point is varied randomly within it's measurement errors. This is often done in weighted regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrap variants: Bayesian\n",
    "\n",
    "Since the bootstrap interprets the data as a kernel estimate of some distribution, in principle it can be fit into a Bayesian analysis. The most obvious route is to attach a weight to each data point encoding how \"real\" it is, with the weights summing to the number of data points.\n",
    "\n",
    "(This is not widely done, since hierarchical mixture models provide a simpler and arguably more natural Bayesian approach.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mixture models\n",
    "\n",
    "This refers to the practice of building a complicated distribution out of simpler components.\n",
    "\n",
    "$p(x) = \\sum_i \\pi_i \\, q_i(x)$,\n",
    "\n",
    "where $\\sum_i \\pi_i=1$ and $q_i(x)$ are normalized PDFs\n",
    "\n",
    "We could generate from this PDF by drawing from $q_i$ with probability $\\pi_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixture models\n",
    "\n",
    "When might we use these?\n",
    "\n",
    "* If the data being modelled really are suspected to consist of multiple populations\n",
    "\n",
    "e.g.\n",
    "- supernova luminosities (without spectroscopic typing)\n",
    "- source vs. background photon energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixture models\n",
    "\n",
    "* If we want a flexible (but still somewhat restricted) model to describe the data\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            e.g., this is a mixture of 3 Gaussians<br>\n",
    "            <img src=\"../graphics/bayes_ci_maxp.png\" width=100%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixture models\n",
    "\n",
    "* If we want a flexible (but still somewhat restricted) model to describe the data\n",
    "\n",
    "How would we decide on the number of mixture components? Depending on the application, we might\n",
    "1. Test how sensitive our inferences are to the number\n",
    "2. Do formal model comparison to decide\n",
    "3. Explicitly marginalize over the number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Non-parametric\" Models\n",
    "\n",
    "The term \"non-parametric\" is used vaguely (and often inaccurately), so it's best explained by example:\n",
    "\n",
    "Example 1:\n",
    "\n",
    "In gravitational lensing, image shear (or stronger distortions) can be measured at the positions of background galaxies in the image plane. Often, the mass of the lens is modelled as the sum of a small number of idealized structures with parametrized mass distributions.\n",
    "\n",
    "Alternatively, [Bradac+ (2005)](http://adsabs.harvard.edu/abs/2005A%26A...437...39B) model the deflection potential on a regular grid, interpolating to the position of measured galaxies, avoiding explicit assumptions about the nature of the lens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Non-parametric\" Models\n",
    "\n",
    "Example 2:\n",
    "\n",
    "In cosmological studies that use distance measurements, the standard technique involves adopting a parametrized model for the energy budget of the Universe ($\\Omega_m$, $\\Omega_\\Lambda$, $w$, etc.) and predicting the distance-redshift relation using that model.\n",
    "\n",
    "However, not everyone is happy with this, and in particular the question of how best to test whether $w$ is constant with time is much discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Non-parametric\" Models\n",
    "\n",
    "e.g.\n",
    "\n",
    "* [Huterer & Starkman (2003)](http://adsabs.harvard.edu/abs/2003PhRvL..90c1301H) advocated a *principle component*-based model for $w(z)$, where the functional forms that the data are most sensitive to are determined and the amplitude of each component is then fit.\n",
    "\n",
    "* More recently, various authors (e.g. [Seikel+ 2012](http://adsabs.harvard.edu/abs/2012JCAP...06..036S)) have used Gaussian Process Regression, a sophisticated interpolation technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Non-parametric\" Models\n",
    "\n",
    "A common feature of these approaches is that they bypass the usual business of defining a physically motivated model. Instead, they usually attempt to define a \"physics-agnostic\" model with just enough felxibility to describe the data.\n",
    "\n",
    "This doesn't avoid the need to make assumptions, but at least it's a different set of assumptions than the first-principles apprach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian (or Stochastic) Processes\n",
    "\n",
    "Our last example used something called a **Gaussian processes**, a particular type of **stochastic process**.\n",
    "\n",
    "A stochastic process is the equivalent of a PDF for functions, i.e. a probability distribution over functions.\n",
    "\n",
    "In other words, if our function of interest is $y(x)$, a stochastic process assigns probabilities $P\\left[y(x)\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Processes\n",
    "\n",
    "A Gaussian processes has the property that\n",
    "\n",
    "$P\\left[y(x) | y(x_1), y(x_2), \\ldots\\right]$\n",
    "\n",
    "is a Gaussian depending on the $x_i$ and $y(x_i)$. The process is specified by a prior for $y(x)$ and a kernel, which basically limits how quickly $y(x)$ can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Processes\n",
    "\n",
    "The upshot is that GP's provide a natural way to incorporate uncertainty into interpolation. With the appropriate assumptions (e.g. Gaussian measurement errors), calculation of the posterior for $y(x)$ is algebraic (no Monte Carlo required).\n",
    "\n",
    "\"GP regression\" simply interprets this interpolating process as a fitting function, complete with posterior uncertainties as a function of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parting thoughts\n",
    "\n",
    "A variety of \"model-independent\" methods exist, and may fit more or less naturally into the principled inference we've been covering.\n",
    "\n",
    "They invariably do correspond to using a model of some form, with a corresponding set of required assumptions.\n",
    "\n",
    "This doesn't mean that they are never appropriate or useful!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
