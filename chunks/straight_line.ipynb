{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference Example: Fitting a Straight Line to the Cepheid Data\n",
    "\n",
    "Goals:\n",
    "\n",
    "* Apply some of the principles discussed in the [\"Bayes Theorem\"](bayes_theorem) and [\"Generative Models\"](generative_models.ipynb) notebooks to a simple example problem involving multiple datapoints.\n",
    "\n",
    "* Carry out a Bayesian inference in `python`, where we derive, code up, evaluate, visualize and summarize the posterior PDF for our model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* [Hogg, Bovy & Lang (2010), \"Data analysis recipes: Fitting a model to data\"](https://arxiv.org/abs/1008.4686)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A period-magnitude relation in Cepheid stars\n",
    "\n",
    "* Cepheids star brightness oscillates with a stable period that appears to be strongly correlated with their mean luminosity (or absolute magnitude).\n",
    "\n",
    "* In the [\"cepheids\"](cepheids.ipynb) notebook we looked at some Cepheid measurements reported by [Riess et al (2011)](https://arxiv.org/abs/1103.2976).\n",
    "\n",
    "* Let's infer the parameters of a simple relationship between Cepheid period and, in the first instance, apparent magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../graphics/cepheid_data.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "* Let's assume that Cepheid stars' luminosities are related to their oscillation periods by a power law, such that their apparent magnitude and log period follow the straight line relation\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;m = a\\;\\log_{10} P + b$\n",
    "\n",
    "* Our task is to infer the parameters $a$ and $b$ given the data, which consist of *observed magnitudes with quoted uncertainties*, such as: \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;m^{\\rm obs} = 24.51 \\pm 0.31$ at $\\log_{10} P = \\log_{10} (13.0/{\\rm days})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Procedure\n",
    "\n",
    "* \"Inferring the parameters $a$ and $b$\" means characterizing the posterior PDF for these parameters, given the data we have $(\\boldsymbol{m}^{\\rm obs})$ and the assumptions we make $(H)$.\n",
    "\n",
    "* i.e., we seek the posterior PDF ${\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Characterizing the posterior PDF\n",
    "\n",
    "* From Bayes Theorem, we have that \n",
    "\n",
    "\n",
    "### $\\;\\;\\;\\;\\;{\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H) = \\frac{{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) {\\rm Pr}(a,b|H)}{{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|H)}$\n",
    "\n",
    "* ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|H)$ is, in this context, a normalization constant\n",
    "\n",
    "* If we can evaluate the numerator of the righthand side of the above expression, we can compute the posterior PDF ${\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H)$ for any choice of parameters $(a,b)$, up to this constant.\n",
    "\n",
    "* This numerator is just the factorization of the joint PDF for all variables that the PGM illustrates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic Graphical Model\n",
    "\n",
    "* Let's draw a PGM for this inverse problem, imagining our way through what we would do to generate a mock dataset like the one we have for each of the 9 NGC galaxy hosts in R11.\n",
    "\n",
    "* If we were generating mock data, then for any plausible choice of parameters $a$ and $b$ we can predict the true magnitude $m_k$ of each star given its period $P_k$, and then add noise to simulate each observed magnitude $m^{\\rm obs}_k$.\n",
    "\n",
    "#### Exercise:  Draw the PGM for this problem with your neighbor, and be prepared to point out its features in 5 minutes' time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"../graphics/pgms_cepheids.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approaching inference\n",
    "\n",
    "* Recall: the PGM illustrates a particular factorization of the joint PDF for all variables in the problem - the one dictated by our model assumptions\n",
    "\n",
    "* In an inverse problem, the data $m^{\\rm obs}$ are constants, fixed by observation\n",
    "\n",
    "* Other parameters of the model are fixed by assumption\n",
    "\n",
    "> The magnitude uncertainties $\\sigma^{\\rm obs}$ are given to us in the data file; we can use them as-is if we believe them. The \"true\" magnitudes $m$ are _determined_ by our power law model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approaching inference\n",
    "\n",
    "* Let's write down the joint PDF corresponding to the above PGM, paying attention to the assumptions involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The sampling distribution ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},H)$\n",
    "\n",
    "* We were given points ($m^{\\rm obs}_k$) with error bars ($\\sigma_k$), which suggests a *Gaussian* sampling distribution for each one:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp{-\\frac{(m^{\\rm obs}_k - m_k)^2}{2\\sigma_k^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note that we are not given the form of the sampling distribution: it has to be assumed. The Gaussian distribution is the _least committal_ (Maximum Entropy) choice given the information provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The sampling distribution ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},H)$\n",
    "\n",
    "* If we assume that the measurements of each Cepheid start are *independent* of each other, then we can define *predicted and observed data vectors* $\\boldsymbol{m}$ and $\\boldsymbol{m}^{\\rm obs}$ (plus a corresponding observational uncertainty vector $\\boldsymbol{\\sigma}$) via:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\boldsymbol{\\sigma},H) = \\prod_k {\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H)$\n",
    "\n",
    "#### Question:  What would the PGM look like if we were unwilling to assume that the datapoints were independent? Discuss this problem with your neighbor, and be prepared to suggest PGM and formula modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The conditional PDF ${\\rm Pr}(m_k|a,b,\\log_{10}{P_k},H)$\n",
    "\n",
    "<img src=\"../graphics/pgms_cepheids.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m_k (modeled mag) is called a **latent** variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The conditional PDF ${\\rm Pr}(m_k|a,b,\\log_{10}{P_k},H)$\n",
    "\n",
    "Our relationship between the intrinsic magnitude and the log period is linear and deterministic, indicating the following *delta-function* PDF:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(m_k|a,b,\\log_{10}{P_k},H) = \\delta(m_k - a\\log_{10}{P_k} - b)$\n",
    "\n",
    "#### Question:  What would it mean to choose a distribution with non-zero width for this PDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Answer it means you have some other systematic errors or intrinsic scatter. this can be another parameter in your model that you can fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The resulting joint likelihood, ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H)$\n",
    "\n",
    "* The PDF for everything inside the PGM plate is the following product:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\sigma,H)\\;{\\rm Pr}(\\boldsymbol{m}|a,b,H)$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = \\prod_k {\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H)\\;\\delta(m_k - a\\log_{10}{P_k} - b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginalizing out the latent variables\n",
    "\n",
    "* The intrinsic magnitudes of each Cepheid $m_k$ are \"latent variables,\" to be _marginalized out_:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\int {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|\\boldsymbol{m},\\sigma,H)\\;{\\rm Pr}(\\boldsymbol{m}|a,b,H)\\; d\\boldsymbol{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = \\prod_k \\int {\\rm Pr}(m^{\\rm obs}_k|m_k,\\sigma_k,H)\\;\\delta(m_k - a\\log_{10}{P_k} - b) dm_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\;\\;\\;\\;\\;\\;\\; \\longrightarrow {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\prod_k {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}_k|(a\\log{P_k} + b),\\sigma_k,H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The log likelihood\n",
    "\n",
    "Taking logs, for numerical stability, the product in the joint likelihood becomes the following sum:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\log {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = \\sum_k \\log {\\rm Pr}(m^{\\rm obs}_k|(a\\log{P_k} + b),\\sigma,H)$\n",
    "\n",
    "which, substituting in our Gaussian form, gives us: \n",
    "\n",
    "$\\;\\log {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) = -\\frac{1}{2}\\sum_k \\log{2\\pi\\sigma_k^2} - \\frac{1}{2} \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*numerical stability* in this case refers to what happens when you want to evaluate low-probability wings of a distribution. small x small = too small for floating point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note that the log likelihood $\\log {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H)$ is a function that can be evaluated, as a function of $a$ and $b$, at constant $\\boldsymbol{m}^{\\rm obs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chi-squared\n",
    "\n",
    "* Astronomers often call the term in the log likelihood that depends on the parameters $\\chi^2$ (\"chi-squared\"):\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\chi^2 = \\sum_k \\frac{(m^{\\rm obs}_k - a\\log{P_k} - b)^2}{\\sigma_k^2}$\n",
    "\n",
    "* $\\chi^2$ is a \"misfit\" statistic, that quantifies the difference between \"observed and predicted data.\" Under our assumptions, it's equal to -2 times the log likelihood (up to a constant). The \"predicted data\" are $m_k = a\\log{P_k} - b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Including the prior ${\\rm Pr}(a,b|H)$\n",
    "\n",
    "* The final pieces of the joint PDF illustrated by the PGM are the PDFs for $a$ and $b$\n",
    "<img src=\"../graphics/pgms_cepheids.png\" width=60% align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Including the prior ${\\rm Pr}(a,b|H)$\n",
    "\n",
    "* The final pieces of the joint PDF illustrated by the PGM are the PDFs for $a$ and $b$, which we can assume to be independent\n",
    "\n",
    "* The joint PDF is:\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(\\boldsymbol{m}^{\\rm obs},a,b|H) = {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) {\\rm Pr}(a|H) {\\rm Pr}(b|H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Since we marginalized out the $m$, analytically, we _could_ have drawn the PGM more simply, jumping directly to ${\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H)$. However, it's often helpful to _explicitly_ distinguish between \"true\" parameters and observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assigning the prior\n",
    "\n",
    "* For now, let's assume a uniform prior PDF for $b$, supposing that we know roughly what size $b$ is (about 20).\n",
    "\n",
    "* Since $a$ is the gradient of a line, let's assume a uniform prior in the angle of inclination $\\theta$ of the line. With $a = \\tan{\\theta}$, this choice corresponds to a Cauchy distribution for $a$.\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(a|H) = \\frac{1}{\\pi}\\frac{1}{1+a^2}\\;\\;{\\rm for}\\;\\; -\\infty < a < +\\infty$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;{\\rm Pr}(b|H) = \\frac{1}{b_{\\rm max} - b_{\\rm min}}$ with $(b_{\\rm min}, b_{\\rm max}) = (10, 30)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Characterizing the posterior PDF\n",
    "\n",
    "* With this completed factorization of the joint PDF for all variables, we have the following product: \n",
    "\n",
    "$\\;\\;{\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H) \\propto {\\rm Pr}(\\boldsymbol{m}^{\\rm obs}|a,b,H) {\\rm Pr}(a|H) {\\rm Pr}(b|H)$\n",
    "\n",
    "* This means that we can evaluate the posterior PDF ${\\rm Pr}(a,b|\\boldsymbol{m}^{\\rm obs},H)$ for any choice of parameters $(a,b)$, up to a normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the posterior PDF\n",
    "\n",
    "We can now code up functions for the log likelihood, the log prior, and the unnormalized log posterior, such that we can evaluate them on a 2D $(a,b)$ parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('../code/cepheids.py').read())\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "\n",
    "data = Cepheids('../examples/Cepheids/R11ceph.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(logP, mobs, sigma, a, b):\n",
    "    return -0.5*np.sum(2*np.pi*sigma**2) - \\\n",
    "            0.5*np.sum((mobs - a*logP - b)**2/(sigma**2))\n",
    "\n",
    "def log_prior(a, b):\n",
    "    amin,amax = -10.0,10.0\n",
    "    bmin,bmax = 10.0,30.0\n",
    "    if (b > bmin)*(b < bmax):\n",
    "        value = np.log(1.0/(bmax-bmin)) - \\\n",
    "                np.log(np.pi) - np.log(1 + a**2)\n",
    "    else:\n",
    "        value = -np.inf\n",
    "    return value\n",
    "\n",
    "def log_posterior(logP, mobs, sigma, a, b):\n",
    "    return log_likelihood(logP,mobs,sigma,a,b) + log_prior(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"log_posterior\" is really \"log_unnormalized_posterior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the posterior PDF\n",
    "\n",
    "Now, let's set up a suitable parameter grid, evaluate the unnormalized log posterior on it, and then renormalize it numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Limits of parameter grids, focused on the high likelihood region:\n",
    "amin, amax = -3.4, -2.4\n",
    "bmin, bmax = 25.7, 26.8\n",
    "limits = (amin, amax, bmin, bmax)\n",
    "\n",
    "def evaluate_posterior_on_a_grid(limits, NGC_ID=4258, npix=100):\n",
    "    \n",
    "    # Make grids:\n",
    "    amin, amax, bmin, bmax = limits\n",
    "    agrid, bgrid, logprob = np.linspace(amin,amax,npix), np.linspace(bmin,bmax,npix), np.zeros([npix,npix])\n",
    "\n",
    "    # Select a Cepheid dataset:\n",
    "    data.select(NGC_ID)\n",
    "\n",
    "    # Loop over parameters, computing unnormlized log posterior PDF:\n",
    "    for i,a in enumerate(agrid):\n",
    "        for j,b in enumerate(bgrid):\n",
    "            logprob[j,i] = log_posterior(data.logP, data.mobs, data.sigma, a, b)\n",
    "\n",
    "    # Exponentiate and normalize to get posterior density:\n",
    "    prob = np.exp(logprob - np.max(logprob))\n",
    "    prob /= np.sum(prob)\n",
    "    \n",
    "    return prob, agrid, bgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prob, a, b = evaluate_posterior_on_a_grid(limits, NGC_ID=4258, npix=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the 2D PDF\n",
    "\n",
    "* Typically we want to be able to see the centroid, size and shape of the posterior PDF\n",
    "\n",
    "* In particular we want to see the _credible regions_ that enclose 68% and 95% of the posterior probability. These are best plotted as contours\n",
    "\n",
    "* Given our assumption that the model is true, the probability that the true values of the model parameters lie within the 95% credible region given the data is 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the 2D PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sorted = np.sort(prob.flatten())\n",
    "C = sorted.cumsum()\n",
    "\n",
    "# Find the pixel values that lie at the levels that contain 68% and 95% of the probability:\n",
    "lvl68 = np.min(sorted[C > (1.0 - 0.68)])\n",
    "lvl95 = np.min(sorted[C > (1.0 - 0.95)])\n",
    "\n",
    "plt.imshow(prob, origin='lower', cmap='Blues', interpolation='none', extent=limits)\n",
    "plt.contour(prob,[lvl95,lvl68],colors='black',extent=limits)\n",
    "plt.grid()\n",
    "plt.xlabel('slope a', fontsize=20)\n",
    "plt.ylabel('intercept b / AB magnitudes', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-inference model checking\n",
    "\n",
    "\n",
    "* Are these inferred parameters sensible? \n",
    "\n",
    "* Let's read off a plausible (a,b) pair and overlay the model period-magnitude relation on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(4258)\n",
    "\n",
    "data.overlay_straight_line_with(a=-2.95, b=26.25, label='Model')\n",
    "\n",
    "data.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing our inferences\n",
    "\n",
    "* Typically, we will want to (or will be expected to) report \"answers\" for our model parameters\n",
    "\n",
    "* This can be difficult: our result _is_ the posterior PDF for the model parameters given the data!\n",
    "\n",
    "* A convenient, and in this case appropriate, choice is to report quantiles of the 1D marginalized PDFs\n",
    "\n",
    "> In general, the most important thing when summarizing inferences is to state clearly what you are doing, preferably with critical commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing our inferences\n",
    "\n",
    "Let's compute the 1D marginalized posterior PDFs for $a$ and for $b$, and report the median and \"68% credible interval\" (defined as the region of 1D parameter space enclosing 68% of the posterior probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_a_given_data = np.sum(prob, axis=0) # Approximate the integral as a sum\n",
    "prob_b_given_data = np.sum(prob, axis=1) # Approximate the integral as a sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that we do have a 1D PDF:\n",
    "print(prob_a_given_data.shape, np.sum(prob_a_given_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1D marginalized posterior PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_1d_marginalized_pdfs(a, b, prob_a_given_data, prob_b_given_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1D marginalized PDF summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"a = \",compress_1D_pdf(a, prob_a_given_data, ci=68, dp=2))\n",
    "\n",
    "print(\"b = \",compress_1D_pdf(b, prob_b_given_data, ci=68, dp=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notes\n",
    "\n",
    "* In this simple case, our report makes sense: the medians of both 1D marginalized PDFs lie within the region of high 2D posterior PDF. *This will not always be the case.*\n",
    "\n",
    "\n",
    "* The marginalized posterior for $x$ has a well-defined meaning, regardless of the higher dimensional structure of the joint posterior:  it is ${\\rm Pr}(x|d,H)$, the PDF for $x$ given the data and the model, and *accounting for the uncertainty in all other parameters*.\n",
    "\n",
    "\n",
    "* The posterior PDF we computed is close to, but not quite, a bivariate Gaussian. What choice of (proper) prior would we have had to make in order for the posterior PDF to be _exactly_ Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: the cost of grid evaluation\n",
    "\n",
    "Brute force computation of poeterior PDFs is computationally expensive. With your partner, work out an approximate expression of the form $t \\sim f(N, P, q)$ for how the CPU time $t$ should scale with \n",
    "\n",
    "* $N$, the number of datapoints\n",
    "\n",
    "* $P$, the number of parameter dimensions ($P=2$ in the straight line example) \n",
    "\n",
    "* $q$, the number of pixels along each dimension of the parameter grid (in the above code $q$ = `npix`)\n",
    "\n",
    "Here, $f$ is some simple function. \n",
    "\n",
    "> You can assume that all floating point operations take the same amount of CPU time, and that this prefactor is unimportant - we are only interested in how the computational expense _scales_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: the cost of grid evaluation\n",
    "\n",
    "Check your scaling with `npix` by re-running the code in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "q = 10\n",
    "prob, a, b = evaluate_posterior_on_a_grid(limits, NGC_ID=4258, npix=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: the cost of grid evaluation\n",
    "\n",
    "Now work out a similar scaling with $N$, $P$ and $q$ for the numerical integration needed to obtain _all $P$ 1D marginalized distributions_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take-home messages\n",
    "\n",
    "* Bayes theorem tells us how to compute the posterior PDF for our model parameters\n",
    "\n",
    "\n",
    "* Sets of independent measurements lead to simple products of likelihood terms\n",
    "\n",
    "\n",
    "* For numerical stability we always work with the _log_ likelihood and _log_ posterior\n",
    "\n",
    "\n",
    "* Evaluating and marginalizing the posterior PDF on a numerical grid is simple but inefficient"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
